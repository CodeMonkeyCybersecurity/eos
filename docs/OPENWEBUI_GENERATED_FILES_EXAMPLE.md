# Open WebUI Generated Files - Example Output

**Generated by:** `eos create openwebui`  
**Mode:** LiteLLM Production (DEFAULT)  
**Date:** October 13, 2025

---

## Command Used

```bash
eos create openwebui \
  --azure-endpoint https://myresource.openai.azure.com \
  --azure-deployment gpt-4 \
  --azure-api-key CSPxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

---

## Generated Files

### 1. `/opt/openwebui/docker-compose.yml`

```yaml
# Open WebUI with LiteLLM Docker Compose Configuration
# Generated by Eos - Code Monkey Cybersecurity

version: '3.8'

services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "3000:8080"
    env_file: .env
    volumes:
      - open-webui:/app/backend/data
    networks:
      - webui_network
    depends_on:
      - litellm-proxy
    environment:
      # Connect Open WebUI to LiteLLM proxy
      - OPENAI_API_BASE_URL=http://litellm-proxy:4000
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}

  litellm-proxy:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-proxy
    restart: unless-stopped
    ports:
      - "4000:4000"
    env_file: .env
    depends_on:
      - litellmproxy_db
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@litellmproxy_db:5432/${POSTGRES_USER}
    command: ["--config", "/app/config.yaml", "--detailed_debug", "--num_workers", "4"]
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    networks:
      - webui_network

  litellmproxy_db:
    image: postgres:17.2-alpine3.21
    container_name: postgresql
    restart: unless-stopped
    env_file: .env
    shm_size: 96mb
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - webui_network

volumes:
  postgres_data:
  open-webui:

networks:
  webui_network:
    driver: bridge
```

### 2. `/opt/openwebui/.env`

```env
# Open WebUI with LiteLLM Environment Configuration
# Generated by Eos - Code Monkey Cybersecurity

# Azure OpenAI Configuration
AZURE_API_BASE=https://myresource.openai.azure.com
AZURE_API_KEY=CSPxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
AZURE_API_VERSION=2025-04-01-preview
AZURE_MODEL=azure/gpt-4

# PostgreSQL Configuration
POSTGRES_PASSWORD=Kx9mP2vL8nQ4rT6wY1zB5cD7fG3hJ0k
POSTGRES_USER=litellm

# LiteLLM Configuration
LITELLM_MASTER_KEY=sk-A5bC8dE2fG9hJ3kL6mN1pQ4rS7tU0vW
LITELLM_SALT_KEY=X2yZ5aB8cD1eF4gH7iJ0kL3mN6oP9qR

# Open WebUI Settings
WEBUI_SECRET_KEY=<64-character-auto-generated-secret>
TZ=Australia/Perth

# Open WebUI Connection to LiteLLM
OPENAI_API_BASE_URL=http://litellm-proxy:4000
OPENAI_API_KEY=${LITELLM_MASTER_KEY}
```

### 3. `/opt/openwebui/litellm_config.yaml`

```yaml
# LiteLLM Configuration
# Generated by Eos - Code Monkey Cybersecurity

model_list:
  # Azure OpenAI Models
  - model_name: azure-gpt-4
    litellm_params:
      model: os.environ/AZURE_MODEL
      api_base: os.environ/AZURE_API_BASE
      api_key: os.environ/AZURE_API_KEY
      api_version: os.environ/AZURE_API_VERSION

# Optional: Add more Azure deployments or other providers here
# - model_name: azure-gpt-4-turbo
#   litellm_params:
#     model: azure/your-other-deployment-name
#     api_base: os.environ/AZURE_API_BASE
#     api_key: os.environ/AZURE_API_KEY
#     api_version: os.environ/AZURE_API_VERSION
```

---

## Key Configuration Points

###  AZURE_MODEL Format
```env
AZURE_MODEL=azure/gpt-4  #  CORRECT - includes azure/ prefix
```

###  Open WebUI Connection
```yaml
environment:
  - OPENAI_API_BASE_URL=http://litellm-proxy:4000  #  Uses Docker service name
  - OPENAI_API_KEY=${LITELLM_MASTER_KEY}
```

###  LiteLLM Config Reference
```yaml
model: os.environ/AZURE_MODEL  #  Will resolve to azure/gpt-4 from .env
```

###  Volume Mount
```yaml
volumes:
  - ./litellm_config.yaml:/app/config.yaml  #  Correct path
```

---

## Success Output

```
================================================================================
Open WebUI deployment completed successfully
================================================================================

Access Open WebUI
  url: http://localhost:3000
  port: 3000

Next steps:
  1. Open your browser and go to http://localhost:3000
  2. Create your first user account (will be admin)
  3. Start chatting with Azure OpenAI

 LiteLLM Proxy (Production Mode):
  UI:   http://localhost:4000/ui
  Docs: http://localhost:4000/docs

Production Features Enabled:
  ✓ Cost tracking and usage monitoring
  ✓ Load balancing across multiple models
  ✓ Request logging and analytics
  ✓ Rate limiting and quotas

Useful commands:
  View logs:        docker compose -f /opt/openwebui/docker-compose.yml logs -f
  Stop service:     docker compose -f /opt/openwebui/docker-compose.yml down
  Restart service:  docker compose -f /opt/openwebui/docker-compose.yml restart

Code Monkey Cybersecurity - 'Cybersecurity. With humans.'
================================================================================
```

---

## Verification Checklist

- [x] AZURE_MODEL has `azure/` prefix
- [x] Open WebUI connects to `http://litellm-proxy:4000` (Docker service name)
- [x] LiteLLM config uses `os.environ/AZURE_MODEL`
- [x] Volume mount path is `/app/config.yaml`
- [x] All services on same Docker network
- [x] PostgreSQL configured correctly
- [x] LiteLLM is DEFAULT mode
- [x] All secrets auto-generated

---

## Testing Commands

```bash
# Start services
docker compose -f /opt/openwebui/docker-compose.yml up -d

# Check all containers are running
docker compose -f /opt/openwebui/docker-compose.yml ps

# Test LiteLLM health
curl http://localhost:4000/health

# Test Open WebUI can reach LiteLLM (from inside container)
docker exec openwebui curl http://litellm-proxy:4000/health

# Check logs for errors
docker compose -f /opt/openwebui/docker-compose.yml logs openwebui | grep -i error
docker compose -f /opt/openwebui/docker-compose.yml logs litellm-proxy | grep -i error

# Access Open WebUI
open http://localhost:3000

# Access LiteLLM UI
open http://localhost:4000/ui
```

---

## Summary

**Status:**  READY FOR DEPLOYMENT

All critical issues have been fixed:
-  Open WebUI properly connected to LiteLLM
-  AZURE_MODEL format correct
-  Docker service names correct
-  LiteLLM is default mode
-  All configuration files properly generated

The implementation is now production-ready!
