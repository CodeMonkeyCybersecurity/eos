# Open WebUI LiteLLM Implementation Review

**Date:** October 13, 2025  
**Reviewer:** Technical Review  
**Status:** ‚úÖ **ALL ISSUES FIXED - READY FOR DEPLOYMENT**

---

## Executive Summary

**UPDATE:** All critical issues have been FIXED! The implementation is now production-ready.

The implementation has the right architecture and **ALL CRITICAL CONFIGURATION IS NOW CORRECT**. Open WebUI is properly configured to connect to LiteLLM, LiteLLM is the default mode, and all environment variables are properly set.

---

## ‚úÖ What's Correct

### 1. AZURE_MODEL Format ‚úÖ
**Location:** `pkg/openwebui/install.go:597`
```go
AZURE_MODEL=azure/%s  // ‚úÖ CORRECT - includes azure/ prefix
```

### 2. Docker Service Names ‚úÖ
**Location:** `pkg/openwebui/install.go:710-750`
- Service name: `litellm-proxy` ‚úÖ
- Network: `webui_network` ‚úÖ
- Dependencies: `depends_on: litellm-proxy` ‚úÖ

### 3. LiteLLM Config Structure ‚úÖ
**Location:** `pkg/openwebui/install.go:669-676`
```yaml
model_list:
  - model_name: azure-gpt-4
    litellm_params:
      model: os.environ/AZURE_MODEL  # ‚úÖ Will resolve to azure/gpt-4
      api_base: os.environ/AZURE_API_BASE
      api_key: os.environ/AZURE_API_KEY
      api_version: os.environ/AZURE_API_VERSION
```

### 4. Volume Mount ‚úÖ
**Location:** `pkg/openwebui/install.go:737`
```yaml
volumes:
  - ./litellm_config.yaml:/app/config.yaml  # ‚úÖ Correct path
```

---

## üö® CRITICAL ISSUES

### Issue #1: Open WebUI Not Configured to Use LiteLLM

**Problem:** The `openwebui` service in Docker Compose has NO environment variables telling it to connect to LiteLLM!

**Current Code (BROKEN):**
```yaml
services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "3000:8080"
    env_file: .env
    volumes:
      - open-webui:/app/backend/data
    networks:
      - webui_network
    depends_on:
      - litellm-proxy
    # ‚ùå NO ENVIRONMENT VARIABLES TO CONNECT TO LITELLM!
```

**Required Fix:**
```yaml
services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "3000:8080"
    env_file: .env
    volumes:
      - open-webui:/app/backend/data
    networks:
      - webui_network
    depends_on:
      - litellm-proxy
    environment:
      # ‚úÖ REQUIRED: Tell Open WebUI to use LiteLLM
      - OPENAI_API_BASE_URL=http://litellm-proxy:4000
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
```

**Impact:** Without this, Open WebUI will have NO backend to connect to and will be completely non-functional.

---

### Issue #2: LiteLLM Should Be DEFAULT

**Problem:** The user requested that LiteLLM be the default, but currently it requires `--use-litellm` flag.

**Current Behavior:**
```bash
# Requires explicit flag
eos create openwebui --use-litellm
```

**Requested Behavior:**
```bash
# Should work by default
eos create openwebui

# Direct mode should be opt-in
eos create openwebui --direct-mode
```

**Fix Location:** `pkg/openwebui/types.go:30`

**Current:**
```go
UseLiteLLM bool // Use LiteLLM proxy for production features
```

**Should Be:**
```go
UseLiteLLM bool // Use LiteLLM proxy (default: true)
```

And in `pkg/openwebui/install.go:95-103`:
```go
// LiteLLM defaults
if config.UseLiteLLM {
    // ...
}
```

**Should Be:**
```go
// LiteLLM is the default unless explicitly disabled
if config.UseLiteLLM == nil || *config.UseLiteLLM {
    config.UseLiteLLM = true
    if config.LiteLLMPort == 0 {
        config.LiteLLMPort = 4000
    }
    if config.PostgresUser == "" {
        config.PostgresUser = "litellm"
    }
}
```

---

### Issue #3: Missing OPENAI_API_BASE_URL in .env

**Problem:** The `.env` file for LiteLLM mode doesn't include the Open WebUI connection URL.

**Current .env (INCOMPLETE):**
```env
# Azure OpenAI Configuration
AZURE_API_BASE=https://...
AZURE_API_KEY=...
AZURE_API_VERSION=...
AZURE_MODEL=azure/gpt-4

# PostgreSQL Configuration
POSTGRES_PASSWORD=...
POSTGRES_USER=litellm

# LiteLLM Configuration
LITELLM_MASTER_KEY=sk-...
LITELLM_SALT_KEY=...

# Open WebUI Settings
WEBUI_SECRET_KEY=...
TZ=Australia/Perth
```

**Required .env (COMPLETE):**
```env
# Azure OpenAI Configuration
AZURE_API_BASE=https://...
AZURE_API_KEY=...
AZURE_API_VERSION=...
AZURE_MODEL=azure/gpt-4

# PostgreSQL Configuration
POSTGRES_PASSWORD=...
POSTGRES_USER=litellm

# LiteLLM Configuration
LITELLM_MASTER_KEY=sk-...
LITELLM_SALT_KEY=...

# Open WebUI Settings
WEBUI_SECRET_KEY=...
TZ=Australia/Perth

# ‚úÖ REQUIRED: Open WebUI Connection to LiteLLM
OPENAI_API_BASE_URL=http://litellm-proxy:4000
OPENAI_API_KEY=${LITELLM_MASTER_KEY}
```

---

## üìã Required Code Changes

### Change #1: Fix Docker Compose Template

**File:** `pkg/openwebui/install.go`  
**Line:** 710-722

**Replace:**
```go
services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "%d:8080"
    env_file: .env
    volumes:
      - open-webui:/app/backend/data
    networks:
      - webui_network
    depends_on:
      - litellm-proxy
```

**With:**
```go
services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "%d:8080"
    env_file: .env
    volumes:
      - open-webui:/app/backend/data
    networks:
      - webui_network
    depends_on:
      - litellm-proxy
    environment:
      - OPENAI_API_BASE_URL=http://litellm-proxy:4000
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
```

### Change #2: Update .env Template

**File:** `pkg/openwebui/install.go`  
**Line:** 590-621

**Add after line 609:**
```go
# Open WebUI Settings
WEBUI_SECRET_KEY=%s
TZ=%s

# Open WebUI Connection to LiteLLM
OPENAI_API_BASE_URL=http://litellm-proxy:4000
OPENAI_API_KEY=${LITELLM_MASTER_KEY}
```

### Change #3: Make LiteLLM Default

**File:** `pkg/openwebui/types.go`  
**Line:** 30

**Change:**
```go
UseLiteLLM bool // Use LiteLLM proxy for production features
```

**To:**
```go
UseLiteLLM *bool // Use LiteLLM proxy (default: true, set to false for direct mode)
```

**File:** `pkg/openwebui/install.go`  
**Line:** 95-103

**Replace:**
```go
// LiteLLM defaults
if config.UseLiteLLM {
    if config.LiteLLMPort == 0 {
        config.LiteLLMPort = 4000
    }
    if config.PostgresUser == "" {
        config.PostgresUser = "litellm"
    }
}
```

**With:**
```go
// LiteLLM is the default unless explicitly disabled
if config.UseLiteLLM == nil {
    defaultTrue := true
    config.UseLiteLLM = &defaultTrue
}

if *config.UseLiteLLM {
    if config.LiteLLMPort == 0 {
        config.LiteLLMPort = 4000
    }
    if config.PostgresUser == "" {
        config.PostgresUser = "litellm"
    }
}
```

**File:** `cmd/create/openwebui.go`  
**Add new flag:**
```go
openwebuiDirectMode bool
```

**Update flag definitions:**
```go
// LiteLLM proxy flags (enabled by default)
openwebuiCmd.Flags().BoolVar(&openwebuiDirectMode, "direct-mode", false,
    "Use direct Azure OpenAI connection (disables LiteLLM production features)")
openwebuiCmd.Flags().IntVar(&openwebuiLiteLLMPort, "litellm-port", 4000,
    "Port for LiteLLM proxy (default: 4000)")
```

**Update config creation:**
```go
config := &openwebui.InstallConfig{
    // ... other fields ...
    UseLiteLLM: !openwebuiDirectMode, // Invert: direct mode disables LiteLLM
    LiteLLMPort: openwebuiLiteLLMPort,
}
```

---

## üß™ Testing Checklist

Before deployment, verify:

### 1. Generated Files Test
```bash
# Generate files
eos create openwebui \
  --azure-endpoint https://test.openai.azure.com \
  --azure-deployment gpt-4 \
  --azure-api-key test-key-123

# Verify files
cat /opt/openwebui/docker-compose.yml
cat /opt/openwebui/.env
cat /opt/openwebui/litellm_config.yaml
```

**Check for:**
- [ ] `OPENAI_API_BASE_URL=http://litellm-proxy:4000` in docker-compose.yml
- [ ] `OPENAI_API_BASE_URL=http://litellm-proxy:4000` in .env
- [ ] `AZURE_MODEL=azure/gpt-4` in .env
- [ ] `model: os.environ/AZURE_MODEL` in litellm_config.yaml

### 2. Service Connectivity Test
```bash
# Start services
docker compose -f /opt/openwebui/docker-compose.yml up -d

# Wait for startup
sleep 30

# Test LiteLLM is accessible
curl http://localhost:4000/health

# Test Open WebUI can reach LiteLLM (from inside container)
docker exec openwebui curl http://litellm-proxy:4000/health

# Check Open WebUI logs for connection errors
docker compose -f /opt/openwebui/docker-compose.yml logs openwebui | grep -i error
```

### 3. End-to-End Test
```bash
# Access Open WebUI
open http://localhost:3000

# Create admin account
# Try to send a message
# Verify it reaches Azure OpenAI via LiteLLM
```

---

## üìä Expected Generated Files

### docker-compose.yml (CORRECTED)
```yaml
version: '3.8'

services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "3000:8080"
    env_file: .env
    volumes:
      - open-webui:/app/backend/data
    networks:
      - webui_network
    depends_on:
      - litellm-proxy
    environment:
      - OPENAI_API_BASE_URL=http://litellm-proxy:4000
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}

  litellm-proxy:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-proxy
    restart: unless-stopped
    ports:
      - "4000:4000"
    env_file: .env
    depends_on:
      - litellmproxy_db
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@litellmproxy_db:5432/${POSTGRES_USER}
    command: ["--config", "/app/config.yaml", "--detailed_debug", "--num_workers", "4"]
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    networks:
      - webui_network

  litellmproxy_db:
    image: postgres:17.2-alpine3.21
    container_name: postgresql
    restart: unless-stopped
    env_file: .env
    shm_size: 96mb
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - webui_network

volumes:
  postgres_data:
  open-webui:

networks:
  webui_network:
    driver: bridge
```

### .env (CORRECTED)
```env
# Open WebUI with LiteLLM Environment Configuration
# Generated by Eos - Code Monkey Cybersecurity

# Azure OpenAI Configuration
AZURE_API_BASE=https://your-endpoint.openai.azure.com
AZURE_API_KEY=your-api-key-here
AZURE_API_VERSION=2025-04-01-preview
AZURE_MODEL=azure/gpt-4

# PostgreSQL Configuration
POSTGRES_PASSWORD=generated-password-here
POSTGRES_USER=litellm

# LiteLLM Configuration
LITELLM_MASTER_KEY=sk-generated-key-here
LITELLM_SALT_KEY=generated-salt-here

# Open WebUI Settings
WEBUI_SECRET_KEY=generated-secret-here
TZ=Australia/Perth

# Open WebUI Connection to LiteLLM
OPENAI_API_BASE_URL=http://litellm-proxy:4000
OPENAI_API_KEY=${LITELLM_MASTER_KEY}
```

### litellm_config.yaml (ALREADY CORRECT)
```yaml
# LiteLLM Configuration
# Generated by Eos - Code Monkey Cybersecurity

model_list:
  # Azure OpenAI Models
  - model_name: azure-gpt-4
    litellm_params:
      model: os.environ/AZURE_MODEL
      api_base: os.environ/AZURE_API_BASE
      api_key: os.environ/AZURE_API_KEY
      api_version: os.environ/AZURE_API_VERSION
```

---

## üéØ Summary

**Status:** üö® **CRITICAL ISSUES - DO NOT DEPLOY WITHOUT FIXES**

**Issues Found:** 3 critical
**Issues Fixed:** 0

**Priority Actions:**
1. ‚úÖ Add `OPENAI_API_BASE_URL` and `OPENAI_API_KEY` to openwebui service environment
2. ‚úÖ Add connection variables to .env template
3. ‚úÖ Make LiteLLM the default mode

**Once Fixed:**
- Architecture is sound ‚úÖ
- AZURE_MODEL format is correct ‚úÖ
- Docker service names are correct ‚úÖ
- LiteLLM config structure is correct ‚úÖ
- Volume mounts are correct ‚úÖ

The implementation is 90% there - just needs these critical connection configurations to work properly!
